{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall, AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "Fit the model using **One-hot missing values alone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1141 entries, C_1005 to P_992\n",
      "Columns: 116 entries, bmi_in_cat2_not_overweight to has_comorbidities.2_yes\n",
      "dtypes: int64(116)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('GDSI_OpenDataset_Final_cleaned_OH.csv')\n",
    "# set secret_name as index\n",
    "df.set_index('secret_name', inplace=True)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['ms_type2_relapsing_remitting', 'ms_type2_progressive_MS','ms_type2_other'])\n",
    "Y = df[['ms_type2_relapsing_remitting', 'ms_type2_progressive_MS','ms_type2_other']]\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=123)\n",
    "\n",
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# Ensure target variables are numpy arrays of type float32\n",
    "y_train = y_train.values.astype('float32')\n",
    "y_test = y_test.values.astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(units=[64, 128], dropout_rate=0.2, learning_rate=0.0001, l2_reg=0.01):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First layer\n",
    "    model.add(Dense(units[0], input_dim=X_train.shape[1], activation='relu', \n",
    "                    kernel_regularizer=l2(l2_reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Second layer\n",
    "    model.add(Dense(units[1], activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 128)\n",
      "(32, 128)\n",
      "(64, 64)\n",
      "(64, 128)\n",
      "(128, 128)\n",
      "(16, 16, 128)\n",
      "(16, 32, 128)\n",
      "(16, 64, 64)\n",
      "(16, 64, 128)\n",
      "(32, 32, 64)\n",
      "(32, 32, 128)\n",
      "(32, 64, 64)\n",
      "(32, 64, 128)\n",
      "(64, 64, 64)\n",
      "(64, 64, 128)\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations_with_replacement\n",
    "\n",
    "def generate_neuron_combinations(min_neurons=128, max_neurons=256):\n",
    "    possible_units = [16, 32, 64, 128, 256]\n",
    "    all_combinations = []\n",
    "    \n",
    "    for num_layers in range(2, 4):  # Testing for 1 to 3 layers\n",
    "        for combination in combinations_with_replacement(possible_units, num_layers):\n",
    "            total_units = sum(combination)\n",
    "            if min_neurons <= total_units <= max_neurons:\n",
    "                all_combinations.append(combination)\n",
    "    \n",
    "    return all_combinations\n",
    "\n",
    "# Generate the combinations\n",
    "neuron_combinations = generate_neuron_combinations()\n",
    "\n",
    "# Print the generated combinations\n",
    "for combo in neuron_combinations:\n",
    "    print(combo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early Layers:\n",
    "\n",
    "- The early layers of a neural network are responsible for learning basic features or patterns from the input data. In the context of tabular data, these features could be simple relationships between input variables or basic statistical properties.\n",
    "- With fewer neurons, the network is encouraged to focus on the most salient, or important, features rather than trying to capture every possible nuance. This helps prevent the network from learning irrelevant details or noise in the data.\n",
    "Later Layers:\n",
    "\n",
    "As the network goes deeper, the layers start to learn more complex, higher-level features by combining the simpler features learned in earlier layers.\n",
    "Increasing the number of neurons in these layers allows the network to capture more complex patterns and interactions between the features. This is crucial as the later layers are where the network starts to form more abstract representations that are necessary for making accurate predictions.Smaller Neurons in Early Layers, Larger Neurons in Later Layers:\n",
    "- Feature Extraction: Starting with a smaller number of neurons in the early layers means that the network will begin by focusing on a more compact representation of the input features. This can be beneficial if you believe that your initial input features contain noise or redundant information that you want to filter out before expanding the representation in later layers.\n",
    "- Dimensionality Expansion: As you increase the number of neurons in the subsequent layers, the network has the capacity to learn more complex relationships and interactions between the features that were identified in the earlier layers. This approach is useful when you think that the complexity of patterns increases as you go deeper into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'batch_size': [10, 20, 40, 60],\n",
    "    'epochs': [50, 75, 100],\n",
    "    'model__units': neuron_combinations,\n",
    "    'model__dropout_rate': [0.0, 0.05, 0.1],\n",
    "    'model__learning_rate': [0.00001, 0.0001, 0.001],\n",
    "    'model__l2_reg': [0.001, 0.01, 0.1]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearch the param space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "my_scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision_micro': 'precision_micro',\n",
    "    'recall_micro': 'recall_micro',\n",
    "    'f1_macro': 'f1_macro',\n",
    "    'f1_micro': 'f1_micro',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=6,scoring=my_scoring, refit='f1_micro')\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explore the other best metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy: 0.7958411949685534\n",
      "Best precision: 0.5584794874064375\n",
      "Best recall: 0.44230699266114515\n",
      "Best F1 micro: 0.7958411949685534\n",
      "Best ROC AUC: 0.6958831705308914\n"
     ]
    }
   ],
   "source": [
    "# Access and print best scores for other metrics\n",
    "best_accuracy = max(grid.cv_results_['mean_test_accuracy'])\n",
    "best_precision = max(grid.cv_results_['mean_test_precision_micro'])\n",
    "best_recall = max(grid.cv_results_['mean_test_recall_micro'])\n",
    "best_f1_micro = max(grid.cv_results_['mean_test_f1_micro'])\n",
    "best_f1_macro = max(grid.cv_results_['mean_test_f1_macro'])\n",
    "best_roc_auc = max(grid.cv_results_['mean_test_roc_auc'])\n",
    "\n",
    "print(f\"Best accuracy: {best_accuracy}\")\n",
    "print(f\"Best precision: {best_precision}\")\n",
    "print(f\"Best recall: {best_recall}\")\n",
    "print(f\"Best F1 micro: {best_f1_micro}\")\n",
    "print(f\"Best ROC AUC: {best_roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(grid.cv_results_)\n",
    "# save results to csv\n",
    "results_df.to_csv('GridSearchCV_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO Fit the model using the best grid search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Owner\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 - 0s - 5ms/step - accuracy: 0.6391 - loss: 0.9568\n",
      "Epoch 2/10\n",
      "80/80 - 0s - 600us/step - accuracy: 0.7820 - loss: 0.6173\n",
      "Epoch 3/10\n",
      "80/80 - 0s - 600us/step - accuracy: 0.7995 - loss: 0.5463\n",
      "Epoch 4/10\n",
      "80/80 - 0s - 588us/step - accuracy: 0.8083 - loss: 0.5048\n",
      "Epoch 5/10\n",
      "80/80 - 0s - 575us/step - accuracy: 0.8158 - loss: 0.4747\n",
      "Epoch 6/10\n",
      "80/80 - 0s - 598us/step - accuracy: 0.8296 - loss: 0.4508\n",
      "Epoch 7/10\n",
      "80/80 - 0s - 594us/step - accuracy: 0.8333 - loss: 0.4316\n",
      "Epoch 8/10\n",
      "80/80 - 0s - 597us/step - accuracy: 0.8459 - loss: 0.4084\n",
      "Epoch 9/10\n",
      "80/80 - 0s - 588us/step - accuracy: 0.8446 - loss: 0.4004\n",
      "Epoch 10/10\n",
      "80/80 - 0s - 576us/step - accuracy: 0.8546 - loss: 0.3814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1cd789952a0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a simple model\n",
    "def create_simple_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_simple_model()\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X_train type: {type(X_train)}, dtype: {X_train.dtype}\")\n",
    "print(f\"y_train type: {type(y_train)}, dtype: {y_train.dtype}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
